{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyJ+0AYAen4/Dzhv+tDE2H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarhanFansuri/Tensorflow-for-All/blob/main/Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrPp5R2-tIT_",
        "outputId": "65c66dae-3f20-468d-e923-d7c93f78a966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of data is 1599999\n",
            "shape of data is (1599999, 6)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1599999 entries, 0 to 1599998\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count    Dtype \n",
            "---  ------    --------------    ----- \n",
            " 0   label     1599999 non-null  int64 \n",
            " 1   time      1599999 non-null  int64 \n",
            " 2   date      1599999 non-null  object\n",
            " 3   query     1599999 non-null  object\n",
            " 4   username  1599999 non-null  object\n",
            " 5   text      1599999 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4809e0f282a9>:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[\"label\"][data[\"label\"] == 4] = 1\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "315/315 [==============================] - 515s 2s/step - loss: 0.5747 - accuracy: 0.6877 - val_loss: 0.5213 - val_accuracy: 0.7404\n",
            "Epoch 2/6\n",
            "315/315 [==============================] - 508s 2s/step - loss: 0.5057 - accuracy: 0.7550 - val_loss: 0.5270 - val_accuracy: 0.7261\n",
            "Epoch 3/6\n",
            "315/315 [==============================] - 507s 2s/step - loss: 0.4809 - accuracy: 0.7710 - val_loss: 0.5326 - val_accuracy: 0.7439\n",
            "Epoch 4/6\n",
            " 10/315 [..............................] - ETA: 7:55 - loss: 0.4546 - accuracy: 0.7825"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled15.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/FarhanFansuri/Tensorflow-for-All/blob/main/Untitled15.ipynb\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "data = pd.read_csv(\"./ds_full.csv\", encoding=\"ISO-8859-1\", engine=\"python\")\n",
        "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
        "\n",
        "\"\"\"# Data exploration\"\"\"\n",
        "\n",
        "data.head(5)\n",
        "data.tail(5)\n",
        "\n",
        "print(f\"len of data is {len(data)}\")\n",
        "print(f\"shape of data is {data.shape}\")\n",
        "data.info()\n",
        "np.sum(data.isnull().any(axis=1))\n",
        "\n",
        "data = data[[\"text\", \"label\"]]\n",
        "\n",
        "data[\"label\"][data[\"label\"] == 4] = 1\n",
        "\n",
        "data_pos = data[data[\"label\"] == 1]\n",
        "data_neg = data[data[\"label\"] == 0]\n",
        "\n",
        "data_pos = data_pos.iloc[: int(20000)]\n",
        "data_neg = data_neg.iloc[: int(20000)]\n",
        "\n",
        "data = pd.concat([data_pos, data_neg])\n",
        "data[\"text\"] = data[\"text\"].str.lower()\n",
        "data[\"text\"].tail(5)\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\", \".join(stopwords.words(\"english\"))\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def cleaning_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda text: cleaning_stopwords(text))\n",
        "data[\"text\"].head()\n",
        "\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "\n",
        "\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans(\"\", \"\", punctuations_list)\n",
        "    return text.translate(translator)\n",
        "    data[\"text\"] = data[\"text\"].apply(lambda x: cleaning_punctuations(x))\n",
        "\n",
        "\n",
        "data[\"text\"].head()\n",
        "\n",
        "\n",
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r\"(.)\\1+\", r\"\\1\", text)\n",
        "\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "data[\"text\"] = data[\"text\"].astype(str)\n",
        "\n",
        "# Now apply the cleaning functions\n",
        "data[\"text\"] = data[\"text\"].apply(cleaning_punctuations)\n",
        "data[\"text\"] = data[\"text\"].apply(cleaning_repeating_char)\n",
        "data[\"text\"] = data[\"text\"].apply(lambda x: cleaning_repeating_char(x))\n",
        "data[\"text\"].head()\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "\n",
        "def cleaning_email(data):\n",
        "    return re.sub(\"@[^\\s]+\", \" \", data)\n",
        "\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "data[\"text\"] = data[\"text\"].astype(str)\n",
        "\n",
        "# Now apply the cleaning functions\n",
        "data[\"text\"] = data[\"text\"].apply(cleaning_punctuations)\n",
        "data[\"text\"] = data[\"text\"].apply(cleaning_repeating_char)\n",
        "data[\"text\"] = data[\"text\"].apply(lambda x: cleaning_email(x))\n",
        "data[\"text\"].tail()\n",
        "\n",
        "\n",
        "def cleaning_URLs(data):\n",
        "    return re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \" \", data)\n",
        "\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda x: cleaning_URLs(x))\n",
        "data[\"text\"].tail()\n",
        "\n",
        "\n",
        "def cleaning_numbers(data):\n",
        "    return re.sub(\"[0-9]+\", \"\", data)\n",
        "\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda x: cleaning_numbers(x))\n",
        "data[\"text\"].tail()\n",
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "data[\"text\"] = data[\"text\"].apply(tokenizer.tokenize)\n",
        "data[\"text\"].head()\n",
        "st = nltk.PorterStemmer()\n",
        "\n",
        "\n",
        "def stemming_on_text(data):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return data\n",
        "\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda x: stemming_on_text(x))\n",
        "data[\"text\"].head()\n",
        "lm = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatizer_on_text(data):\n",
        "    text = [lm.lemmatize(word) for word in data]\n",
        "    return data\n",
        "\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda x: lemmatizer_on_text(x))\n",
        "\n",
        "X = data.text\n",
        "y = data.label\n",
        "y\n",
        "\n",
        "max_len = 500\n",
        "tok = Tokenizer(\n",
        "    num_words=2000\n",
        ")  # the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "tok.fit_on_texts(X)\n",
        "sequences = tok.texts_to_sequences(X)\n",
        "sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    sequences_matrix, y, test_size=0.3, random_state=2\n",
        ")\n",
        "\n",
        "\n",
        "def tensorflow_based_model():  # Defined tensorflow_based_model function for training tenforflow based model\n",
        "    inputs = Input(name=\"inputs\", shape=[max_len])  # step1\n",
        "    layer = Embedding(2000, 50, input_length=max_len)(inputs)  # step2\n",
        "    layer = LSTM(128, return_sequences=True)(layer)\n",
        "    layer = LSTM(64)(layer)\n",
        "    layer = Dense(256, name=\"FC1\")(layer)  # step4\n",
        "    layer = Activation(\"relu\")(layer)  # step5\n",
        "    # layer = Dropout(0.5)(layer)  # step6\n",
        "    layer = Dense(1, name=\"out_layer\")(\n",
        "        layer\n",
        "    )  # step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n",
        "    layer = Activation(\"sigmoid\")(\n",
        "        layer\n",
        "    )  # step5 but this time activation function is sigmoid for only one output.\n",
        "    model = Model(\n",
        "        inputs=inputs, outputs=layer\n",
        "    )  # here we are getting the final output value in the model for classification\n",
        "    return model  # function returning the value when we call it\n",
        "\n",
        "\n",
        "model = tensorflow_based_model()\n",
        "# here we are calling the function of created model\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.9\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, Y_train, batch_size=80, epochs=6, validation_split=0.1\n",
        ")  # here we are starting the training of model by feeding the training data\n",
        "print(\"Training finished !!\")\n",
        "\n",
        "\n",
        "# Plot training history\n",
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history[\"accuracy\"])\n",
        "    plt.plot(history.history[\"val_accuracy\"])\n",
        "    plt.title(\"Model accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history[\"loss\"])\n",
        "    plt.plot(history.history[\"val_loss\"])\n",
        "    plt.title(\"Model loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Call the function with the training history\n",
        "plot_history(history)\n"
      ]
    }
  ]
}